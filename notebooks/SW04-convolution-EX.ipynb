{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolution, filtering and correlation**\n",
    "\n",
    "\n",
    "<div style=\"color:#777777;margin-top: -15px;\">\n",
    "<b>Author</b>: Norman Juchler |\n",
    "<b>Course</b>: ADLS ISP |\n",
    "<b>Version</b>: v1.2 <br><br>\n",
    "<!-- Date: 13.03.2025 -->\n",
    "<!-- Comments: Fully refactored. -->\n",
    "</div>\n",
    "\n",
    "In the lecture, we discussed how convolution is a fundamental operation in signal processing. But why is it so important? Let's explore its significance further in this exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "Let's begin with the usual preparatory steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.fft import (fft, ifft, fftfreq, fftshift,\n",
    "                       rfft, irfft, rfftfreq)\n",
    "import soundfile as sf\n",
    "import scipy.signal\n",
    "\n",
    "# For audio playback\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import isp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "\n",
    "## **&#9734;  Exercise 1 – Filter a noisy signal**\n",
    "\n",
    "In this exercise, we will address noise in a signal using a simple windowed average filter. This is done by convolving the discrete-time signal, $x[n]$, with a window function. A window function is defined as a function that is zero outside a specified interval and non-zero within that interval. When convolving a signal with a window function, we effectively average the signal over that interval.\n",
    "\n",
    "This process is a form of **time-domain filtering**, where the signal is manipulated directly in the time domain using the windowed function. The window function is also referred to as the **kernel** of the filter.\n",
    "\n",
    "### **Instructions**: \n",
    "* Read the audio signal from a file.\n",
    "* Add noise to the signal.\n",
    "* Choose a window function. You can find a list of available options [here](https://docs.scipy.org/doc/scipy/reference/signal.windows.html).\n",
    "* Convolve the noisy signal with the window function.\n",
    "* Visualize and compare the original signal, the noisy signal and the filtered signal\n",
    "* Analyze the amplitude spectrum of:\n",
    "  * (The original signal)\n",
    "  * The noisy signal\n",
    "  * The filtered signal\n",
    "  * The window function itself\n",
    "* Discuss the result:\n",
    "  * Can you hear a difference between the original and filtered signals? If so, can you explain the reason?\n",
    "  * Try different window functions. What differences do you observe?\n",
    "  * How does changing the window width affect the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# 1) Load the audio file\n",
    "audio_file = \"../data/signals/fail.mp3\"\n",
    "x, sample_rate = isp.load_audio(audio_file, ensure1d=1)\n",
    "\n",
    "# 2) Add noise to the signal\n",
    "noise_std = 0.02    # Standard deviation of the noise\n",
    "np.random.seed(0)   # Set seed for reproducibility\n",
    "noise = np.random.normal(loc=0.0, \n",
    "                         scale=noise_std, \n",
    "                         size=x.shape)\n",
    "x_noisy = x + noise\n",
    "\n",
    "# 3) Create the window filter\n",
    "width = 100\n",
    "window = ...\n",
    "\n",
    "# 4) Apply the filter to the noisy signal\n",
    "x_filtered = ...\n",
    "\n",
    "# 5) Listen to the results\n",
    "display(\"Original\")\n",
    "display(Audio(x, rate=sample_rate))\n",
    "display(\"Noisy\")\n",
    "display(Audio(x_noisy, rate=sample_rate))\n",
    "display(\"Filtered\")\n",
    "display(Audio(x_filtered, rate=sample_rate))\n",
    "\n",
    "# 6) Visualize the signals in the time domain\n",
    "...   # Plot the original, noisy, and filtered signals in the time domain\n",
    "\n",
    "# 7) Visualize the signals in the frequency domain\n",
    "...   # Plot the amplitude spectrum of the original, noisy, and filtered signals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise2'></a>\n",
    "\n",
    "## **&#9734; Exercise 2 – Explore the convolution**\n",
    "\n",
    "In this exercise, we compare different methods for performing the convolution of two signals in Python. Let's first recall the definition of the convolution for discrete-time signals:  \n",
    "\n",
    "$$ y[n] = \\sum_{k=0}^{K-1} h[k] \\cdot x[n-k]$$\n",
    "\n",
    "where $x$ is a signal with $N$ samples, and $h$ is a filter of length $K$. For a deeper understanding, refer to [this section](https://brianmcfee.net/dstbook-site/content/ch03-convolution/Convolution.html#) of Brian McFee’s excellent online book on **Digital Signal Theory**.  \n",
    "\n",
    "We will explore four different approaches to implementing the convolution:  \n",
    "\n",
    "\n",
    "- **Method 1**: Using a for-loop to manually compute the convolution (see lecture slides).  \n",
    "- **Method 2**: Using NumPy’s [`np.convolve()`](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html).  \n",
    "- **Method 3**: Applying the Fourier Transform and the convolution theorem.  \n",
    "- **Method 4**: Using SciPy’s [`scipy.signal.convolve()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html).  \n",
    "\n",
    "Compare the results of the four methods and measure the execution time of each. Which method is the fastest?  \n",
    "\n",
    "### **Instructions:**\n",
    "* Implement the four methods (use the provided code structure).  \n",
    "* Run the prepared comparison code and analyze the results.  \n",
    "\n",
    "**Hint:** When using `np.convolve()` and `scipy.signal.convolve()`, set the mode to \"full\" to obtain the complete convolution output.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Create a simple signal (white noise)\n",
    "N = int(1e4)\n",
    "x = np.random.randn(N)\n",
    "\n",
    "# Create a simple filter\n",
    "K = 1000\n",
    "h = scipy.signal.windows.hann(K)\n",
    "\n",
    "def method1(h, x):\n",
    "    \"\"\"Method 1: Use a for loop\"\"\"    \n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "def method2(h, x):\n",
    "    \"\"\"Method 2: Use np.convolve()\"\"\"\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "def method3(h, x):\n",
    "    \"\"\"Method 3: Use FFT and the convolution theorem\"\"\"\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "def method4(h, x):\n",
    "    \"\"\"Method 4: Use scipy.signal.convolve()\"\"\"\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "\n",
    "# Compare the different methods\n",
    "import time\n",
    "methods = [method1, method2, method3, method4]\n",
    "method_names = [\"for loop\", \"np.convolve\", \"FFT\", \"scipy.signal.convolve\"]\n",
    "timings = []\n",
    "y_ref = scipy.signal.convolve(x, h, mode=\"full\")\n",
    "y_rets = []\n",
    "for method, method_name in zip(methods, method_names):\n",
    "    print(f\"Running method: {method_name}\")\n",
    "    tic = time.time()\n",
    "    #timing = %timeit -o method(h, x)\n",
    "    y = method(h, x)\n",
    "    y_rets.append(y)\n",
    "    toc = time.time()\n",
    "    timings.append(toc-tic)\n",
    "    plt.plot(y, label=method_name)\n",
    "\n",
    "plt.plot(y_ref, \"k:\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Timings:\")\n",
    "for method, timinig in zip(method_names, timings):\n",
    "    print(\"%-25s %.3fms\" % (method, timing.best*1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Discussion**\n",
    "\n",
    "Exemplary timing results:\n",
    "\n",
    "```lang-none\n",
    "   for loop                  2215.081ms\n",
    "   np.convolve                  4.034ms\n",
    "   FFT                          0.187ms\n",
    "   scipy.signal.convolve        0.196ms\n",
    "```\n",
    "\n",
    "This exercise demonstrates the significant performance advantage of using the FFT for computing convolutions. The FFT-based method is substantially faster than other approaches, particularly for large signals and filters.\n",
    "\n",
    "The for-loop implementation is the slowest, as pure Python is not optimized for vectorized calculations. In contrast, `np.convolve()` employs a more conventional numerical approach to approximate the convolution integral and benefits from vectorization. However, it is still far less efficient than the FFT-based method.\n",
    "\n",
    "Despite the differences in computational speed, all methods produce results with comparable accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise3'></a>\n",
    "\n",
    "## **&#9734; Exercise 3 – Impulse response**\n",
    "\n",
    "Let's recall the **impulse function**, which we encountered in the lecture. It is a function that is zero everywhere except at $t=0$, where it has a value such that its integral equals one. For discrete-time signals, the impulse function is defined as:  \n",
    "\n",
    "$$\\delta[n]= [1, 0, 0, 0, \\dots]$$  \n",
    "\n",
    "If we use $x[n] = \\delta[n]$ in a convolution operation, we obtain the following:  \n",
    "\n",
    "$$\\begin{align*}  \n",
    "y[n] &= \\sum_{k=0}^{K-1} h[k] \\cdot x[n-k] \\\\  \n",
    "&= h[n] \\cdot \\delta[0] \\\\  \n",
    "&= h[n]  \n",
    "\\end{align*}$$ \n",
    "\n",
    "In simpler terms: When we apply an impulse function $\\delta(t)$ to a filter or system $h(t)$ (or its discrete-time counterpart $h[n]$), we directly obtain $h(t)$ itself! This function $h(t)$ is called the **impulse response** of the system. The impulse response fully characterizes a convolutional filter—no additional information is needed.  \n",
    "\n",
    "If we want to understand how an *<u>unknown</u>* system transforms an input signal $x(t)$, we can measure the system's impulse response $h(t)$ and then convolve it with  $x(t)$.\n",
    "\n",
    "In this exercise, we explore the concept of impulse response and convolution by implementing the **reverberation effect**. Reverberation (German: *Nachhall*) is the persistence of sound in a space or room after the original sound has stopped, caused by multiple reflections within the room.  \n",
    "\n",
    "To simulate what an audio signal would sound like in a specific room (e.g., a **cathedral**), we can apply a **reverb effect** by convolving the audio signal with the room’s impulse response.  But how do we obtain the impulse response of a room? It can be measured by playing an impulse sound (e.g., a clap or a gunshot) in the room and recording the response with a microphone. (Alternatively, it can also be simulated using mathematical models. We do not cover this here.)\n",
    "\n",
    "Let's try this out!  \n",
    "\n",
    "\n",
    "\n",
    "### **Instructions**  \n",
    "- Read these articles on [reverberation (Wikipedia)](https://en.wikipedia.org/wiki/Reverberation) and [impulse response (Brian McFee, Digital Signal Theory)](https://brianmcfee.net/dstbook-site/content/ch03-convolution/IR.html#).  \n",
    "- Load the audio signal $x[n]$ and the impulse response of a room $h[n]$ (three different alternatives are available).  \n",
    "- Apply the convolution $h * x[n]$ to generate the reverb effect.\n",
    "- Listen to the result—does it sound natural? Do you like the effect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# 1) Read in the impulse response\n",
    "import soundfile as sf\n",
    "ir, sample_rate = isp.load_audio(\"../data/signals/rir-01.mp3\", ensure1d=True)\n",
    "#ir, sample_rate = isp.load_audio(\"../data/signals/rir-02.mp3\", ensure1d=True)\n",
    "ir, sample_rate = isp.load_audio(\"../data/signals/rir-03.mp3\", ensure1d=True)\n",
    "\n",
    "# 2) Read the audio signal for which we want to apply the reverb\n",
    "x, sample_rate = isp.load_audio(\"../data/signals/fail.mp3\", ensure1d=True)\n",
    "\n",
    "# 3) Convolve the audio signal with the impulse response\n",
    "y = ...\n",
    "\n",
    "# 4) Listen to the original and the reverberated audio signal\n",
    "display(\"Original signal\")\n",
    "display(Audio(x, rate=sample_rate))\n",
    "display(\"Impulse response\")\n",
    "display(Audio(ir, rate=sample_rate))\n",
    "display(\"Reverb result\")\n",
    "display(Audio(y, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise4'></a>\n",
    "\n",
    "## **&#9734; Exercise 4 – Auto-correlation**\n",
    "\n",
    "**Cross-correlation** measures the similarity between two different signals as a function of their displacement (or lag):  \n",
    "\n",
    "$$R_{xy}(t) = \\int_{-\\infty}^{\\infty} x(\\tau)\\cdot y(\\tau-t)\\;d\\tau$$\n",
    "\n",
    "**Autocorrelation**, a special case of cross-correlation, measures the similarity between a signal and a time-shifted (delayed) version of itself, meaning we set $y(t) = x(t)$ in the above equation:  \n",
    "\n",
    "$$R_{xx}(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\cdot x(\\tau-t)\\;d\\tau$$\n",
    "\n",
    "Autocorrelation is useful for detecting periodicity in a signal and estimating parameters such as time lag or signal delay. \n",
    "\n",
    "In discrete-time form, autocorrelation is expressed as:  \n",
    "\n",
    "$$R_{xx}[m] = \\sum_{n=-\\infty}^{\\infty} x[n] \\cdot x[n-m]$$  \n",
    "\n",
    "To improve interpretability, we normalize the autocorrelation A) by subtracting the mean and B) by scaling by the maximum value (which corresponds to the variance of the signal, but we omit this detail here). This ensures that the autocorrelation is not dominated by the mean of the signal.\n",
    "\n",
    "$$R_{xx}[m] = \\frac{1}{\\sigma_x^2} \\sum_{n=-\\infty}^{\\infty} \\left(x[n] - \\mu_x \\right) \\cdot \\left(x[n-m] - \\mu_x\\right)$$  \n",
    "\n",
    "where $\\mu_x$ and $\\sigma_x$ are the sample mean and standard deviation of the signal $x$, respectively.  \n",
    "\n",
    "With a **peak-finding algorithm**, autocorrelation (and cross-correlation) can be used to **detect periodicity** in data.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In the module *Biomedical Sensors and Imaging*, you learned how to collect data from wearable sensors. Specifically, you explored how *accelerometer and gyroscope measurements can be used to detect motion patterns. Autocorrelation can help identify such repetitive patterns.  \n",
    "\n",
    "A similar approach can be applied to audio data to analyze rhythmic patterns.  \n",
    "\n",
    "### **Instructions**\n",
    "* Load timeseries data:\n",
    "   * Acceleration data (at 52Hz)\n",
    "   * Gyroscopic data (52Hz)\n",
    "   * Audio data\n",
    "* Visualize the data in the time domain.  \n",
    "* Compute and plot the autocorrelation of the signal.  \n",
    "* Can you infer the motion pattern?\n",
    "* Compute and visualize the *amplitude spectrum for comparison.  \n",
    "* Discuss your findings. How do the results compare across different datasets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Nothing to do, this time :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Load data (choose one)\n",
    "#dataset = \"acc-stairs\"     # Try to infer the walking rhythm!\n",
    "#dataset = \"acc-walking\"    # ...ditto\n",
    "#dataset = \"gyro-stairs\"   # ...\n",
    "#dataset = \"gyro-walking\"  # ...\n",
    "dataset = \"heart-beat\"    # Try to infer the heart rate! Is it regular?\n",
    "#dataset = \"drums\"         # Try to infer the rhythm! Is it synthetic or real?\n",
    "#dataset = \"waves\"         # Is there a rhythm?\n",
    "#dataset = \"noise\"         # Do you recognize the type of function?\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    \"\"\"Utility function to load the data for the given dataset.\"\"\"\n",
    "    if dataset in (\"acc-stairs\", \"gyro-stairs\"):\n",
    "        # Accelerometer or gyrometer data of a person walking \n",
    "        # up and down the stairs\n",
    "        fs = 52\n",
    "        df = pd.read_csv(\"../data/data_stairs.csv\")\n",
    "        # Pick the first quarter of the data (has clearer rhythm)\n",
    "        df = df.head(len(df)//4)\n",
    "        x = df[\"acc\"] if dataset==\"acc-stairs\" else df[\"gyro\"]\n",
    "        is_audio = False\n",
    "    elif dataset in (\"acc-walking\", \"gyro-walking\"):\n",
    "        # Accelerometer or gyrometer data of a person walking\n",
    "        fs = 52\n",
    "        df = pd.read_csv(\"../data/data_walking.csv\")\n",
    "        # Pick the first quarter of the data (has clearer rhythm)\n",
    "        df = df.head(len(df)//4)\n",
    "        x = df[\"acc\"] if dataset==\"acc-stairs\" else df[\"gyro\"]\n",
    "        is_audio = False\n",
    "    elif dataset == \"heart-beat\":\n",
    "        # Sound of a heart beat.\n",
    "        x, fs = isp.load_audio(\"../data/signals/heart-beat2.mp3\", ensure1d=True)\n",
    "        is_audio = True\n",
    "    elif dataset == \"drums\":\n",
    "        # Sound of some drums.\n",
    "        x, fs = isp.load_audio(\"../data/signals/drums.mp3\", ensure1d=True)\n",
    "        is_audio = True\n",
    "    elif dataset == \"metronome\":\n",
    "        # Sound of a metronome.\n",
    "        x, fs = isp.load_audio(\"../data/signals/metronome.mp3\", ensure1d=True)\n",
    "        is_audio = True\n",
    "    elif dataset == \"waves\":\n",
    "        # Sound of waves.\n",
    "        x, fs = isp.load_audio(\"../data/signals/waves.mp3\", ensure1d=True)\n",
    "        is_audio = True\n",
    "    elif dataset == \"noise\":\n",
    "        fs = 44100\n",
    "        duration = 10\n",
    "        x = np.random.randn(int(duration*fs))\n",
    "        x = x / np.max(np.abs(x))\n",
    "        is_audio = True\n",
    "    return x, fs, is_audio\n",
    "    \n",
    "\n",
    "# Load the data\n",
    "x, fs, is_audio = load_data(dataset)\n",
    "\n",
    "# Compute corresponding time vector\n",
    "t = np.arange(x.size) / fs\n",
    "\n",
    "if is_audio:\n",
    "    # Let's listen to the data if it is audio.\n",
    "    display(\"Dataset: \" + dataset)\n",
    "    display(Audio(x, rate=fs))\n",
    "\n",
    "# Visualization (with plotly, as it offers interactive plots)\n",
    "duration = x.size / fs\n",
    "x_view = x[:int(10*fs)] if duration > 10 else x\n",
    "t_view = t[:int(10*fs)] if duration > 10 else t\n",
    "colors = isp.PALETTE_PLOTLY\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t_view, y=x_view, \n",
    "                         mode=\"lines\", name=dataset, \n",
    "                         line=dict(color=colors[0])))\n",
    "fig.update_layout(title=\"Time signal (dataset:\" + dataset + \")\", \n",
    "                  xaxis_title=\"Time [s]\")\n",
    "fig.show()\n",
    "\n",
    "# Here we compute the autocorrelation:\n",
    "# Why is the autocorrelation useful?\n",
    "# It allows to infer the rhythm of a signal.\n",
    "\n",
    "# Normalize the signal to have zero mean. (Otherwise, the autocorrelation\n",
    "# will be dominated by the mean.)\n",
    "x = x - x.mean()\n",
    "# Compute the autocorrelation\n",
    "x_corr = scipy.signal.correlate(x, x, mode=\"full\")\n",
    "# Let's normalize the result by the maximum value\n",
    "# (which is equivalent to the variance of the signal)\n",
    "x_corr = x_corr / x_corr.max()\n",
    "# Finally, we only keep the positive lags, as the autocorrelation\n",
    "# function is symmetric with respect to the vertical axis.\n",
    "x_corr = x_corr[x_corr.size//2:]\n",
    "\n",
    "# Plot the autocorrelation\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(x_corr.size)/fs, \n",
    "                         y=x_corr, \n",
    "                         mode=\"lines\", \n",
    "                         name=\"Autocorrelation\", \n",
    "                         line=dict(color=colors[0])))\n",
    "fig.update_layout(title=\"Autocorrelation (dataset: \" + dataset + \")\",\n",
    "                   xaxis_title=\"Lag (s)\", yaxis_title=\"Autocorrelation\")\n",
    "\n",
    "\n",
    "# Find peaks (tuned for audio signals)\n",
    "if is_audio:\n",
    "    x_peaks, _ = scipy.signal.find_peaks(x_corr, prominence=0.1, distance=fs/10)\n",
    "    fig.add_trace(go.Scatter(x=x_peaks/fs, y=x_corr[x_peaks], \n",
    "                            mode=\"markers\", \n",
    "                            marker=dict(color=colors[1]),\n",
    "                            name=\"Peaks\"))\n",
    "fig.show()\n",
    "\n",
    "# Let's finally visualize the spectrum of the signal\n",
    "xf = np.abs(rfft(np.asarray(x), n=len(x)))\n",
    "# Ensure an amplitude preserving spectrum\n",
    "xf = xf / xf.max()\n",
    "f = rfftfreq(x.size, 1/fs)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=f, y=xf,\n",
    "                         mode=\"lines\", name=\"Spectrum\",\n",
    "                         line=dict(color=colors[0])))\n",
    "fig.update_layout(title=\"Spectrum (dataset: \" + dataset + \")\",\n",
    "                  xaxis_title=\"Frequency (Hz)\", yaxis_title=\"Magnitude\")\n",
    "#if is_audio:\n",
    "    #fig.update_xaxes(range=[0, 400])\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "* (Note: The PDF version does not show the plotly plots!)\n",
    "* White noise, which can be approximated as `x = np.random.randn()`, has an autocorrelation function that resembles an impulse function. As the number of samples increases, the width of this impulse converges to zero. This occurs because the signal is inherently uncorrelated with itself at any lag.\n",
    "* Some of the audio data appears to be synthetically generated, as indicated by an exceptionally regular autocorrelation function. The drum solo and heartbeat signals exhibit this characteristic, suggesting they may have been artificially synthesized. In contrast, the metronome recording is likely a real recording, as its autocorrelation pattern deviates slightly from perfect regularity.\n",
    "* While main frequencies are clearly visible in the amplitude spectrum, discerning the rhythm from the spectrum alone can be challenging. A rhythm is typically formed by periodic bursts of sound or movement. In music, for example, instruments produce sounds at specific frequencies, and even drums have an inherent pitch. These sound frequencies dominate the amplitude spectrum (power spectrum) since they carry most of the signal's energy. However, rhythmic patterns are harder to detect in the frequency domain, as they arise from periodic modulation of signal amplitude rather than distinct frequency components.\n",
    "* A more effective way to visualize the time-varying frequency content of a signal is by using the Short-Time Fourier Transform (STFT) and its corresponding spectrogram, which provide a clearer representation of how frequencies evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import librosa\n",
    "except ImportError:\n",
    "    print(\"Installing librosa...\")\n",
    "    %pip install librosa\n",
    "    \n",
    "import librosa\n",
    "\n",
    "if is_audio:\n",
    "    S = librosa.stft(x)\n",
    "else:\n",
    "    # n_fft should be smaller than the signal length...\n",
    "    n_fft = 128\n",
    "    print(\"Signal length:\", x.size)\n",
    "    print(\"Sampling rate:\", fs)\n",
    "    print(\"Number of samples per window (n_fft):\", n_fft)\n",
    "    S = librosa.stft(x.values, n_fft=n_fft)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "S_amp = np.abs(S)\n",
    "S_dB = librosa.amplitude_to_db(S_amp, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, \n",
    "                               x_axis='time',\n",
    "                               y_axis='log', \n",
    "                               sr=fs, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set_title(\"Spectrogram\", weight=\"bold\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-isp-fs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
