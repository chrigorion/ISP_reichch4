{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Digital image processing – Basics**\n",
    "\n",
    "<div style=\"color:#777777;margin-top: -15px;\">\n",
    "<b>Author</b>: Norman Juchler |\n",
    "<b>Course</b>: ADLS ISP |\n",
    "<b>Version</b>: v1.2 <br><br>\n",
    "<!-- Date: 30.03.2025 -->\n",
    "<!-- Comments: Entirely refactored -->\n",
    "</div>\n",
    "\n",
    "In this notebook, we will learn how to handle images in Python using the OpenCV and PIL packages. \n",
    "We will also learn how to read, create and modify images using these libraries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "Let's begin with the usual preparatory steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "# Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable this line if you want to use the interactive widgets\n",
    "# It requires the ipympl package to be installed.\n",
    "#%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import isp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "\n",
    "## **&#9734;  Exercise 1 – Image representation**\n",
    "We have already learned that a digital grayscale image can be represented as a 2D array of pixel intensity values. In this exercise, we will learn how to load an image and access its pixel values using the OpenCV library. OpenCV is a powerful toolkit for image processing and computer vision, particularly well-suited for real-time applications. It is written in C++ but provides convenient Python bindings.\n",
    "\n",
    "A grayscale image is stored as a 2D array with dimensions corresponding to its height ($h$) and width ($w$), resulting in $h \\times w$ intensity values. In contrast, a color image (e.g., RGB) is typically represented as a 3D array, where each pixel has three values – one for each color channel – resulting in $3 \\times h \\times w$ values. In Python, the most suitable data structure for this kind of data is a NumPy array.\n",
    "\n",
    "In fact, most image processing libraries in Python either natively use NumPy arrays to represent images or provide easy ways to convert to and from NumPy arrays. This compatibility makes it convenient to combine different libraries within the same workflow.\n",
    "\n",
    "To display an image, we can use the [`imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) function from matplotlib.\n",
    "\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "* Load the image \"../data/images/kingfisher-gray.jpg\" using the function `cv.imread()`. See [this example](https://docs.opencv.org/4.x/db/deb/tutorial_display_image.html) for reference.\n",
    "* Inspect the returned array: What is its shape? What is its data type?\n",
    "* Display the image using [`plt.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html). Hint: Set the argument `cmap=\"gray\"` for proper grayscale visualization.\n",
    "* Retrieve the pixel value at position (100, 100).\n",
    "* Select a region of interest (ROI) starting at pixel (75, 200) with a width of 200 and height of 100 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# Load the image\n",
    "gray = cv.imread(...)\n",
    "\n",
    "# Print the image properties\n",
    "print(\"Height, width:       \", ...)\n",
    "print(\"Data type:           \", ...)\n",
    "print()\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(...)\n",
    "plt.show()\n",
    "\n",
    "# Read the pixel at [100,100]\n",
    "print(\"Pixel at [100,100]:  \", ...)\n",
    "\n",
    "# Crop the image and display it\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "\n",
    "## **&#9734;  Exercise 2 – Digitization of images**\n",
    "\n",
    "Just like time-based signals, digital images are represented as arrays of numbers. The digitization (or discretization) process involves two key steps:\n",
    "\n",
    "1. **Sampling**: The continuous image is sampled at regular spatial intervals to create a grid of pixels.\n",
    "2. **Quantization**: The continuous range of pixel intensities is mapped to a finite set of discrete levels.\n",
    "\n",
    "The result is a 2D array where each value corresponds to the intensity (or color) of a pixel.\n",
    "\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "* **Theory**: Compare sampling in time vs. space\n",
    "  * Explain the key differences between sampling continuous-time signals (e.g., audio) and spatial sampling in images.\n",
    "  * Consider aspects such as dimensionality, interpretation of sample points, and typical challenges in each domain.\n",
    "* **Quantization**: Reducing image depth\n",
    "  * What happens when you reduce the number of bits used to represent each pixel?\n",
    "  * Use the image from the previous exercise.\n",
    "  * Study the function quantize(image, nbits) and make sure you understand how it works.  \n",
    "    Note: NumPy does not natively support arbitrary n-bit data types (e.g., 4-bit, 5-bit, etc.), so we simulate quantization using 8-bit arrays by rounding values appropriately.\n",
    "  * Use the quantize() function to create lower bit-depth versions of the image and observe the visual impact.\n",
    "* **Spatial sampling**: Reducing resolution\n",
    "  * What happens when you reduce the number of pixels?\n",
    "  * Subsample the image progressively by a factor of 2 using slicing, e.g., `gray[::2, ::2]`.\n",
    "  * Reflect: Aside from lower resolution, what other effects do you expect with this type of downsampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(image, nbits):\n",
    "    \"\"\"\n",
    "    Method 1:\n",
    "    Quantize an image to a lower bit depth. The image must be in uint8 format,\n",
    "    and the number of bits must be between 1 and 8 (inclusive).\n",
    "    Works for grayscale and multi-channel (e.g., RGB) images.\n",
    "    \"\"\"\n",
    "    assert image.dtype == np.uint8, \"Image must be of type uint8\"\n",
    "    assert 1 <= nbits <= 8, \"Number of bits must be between 1 and 8\"\n",
    "    \n",
    "    # Define bin edges to divide the 256 intensity levels\n",
    "    # Example bins:\n",
    "    #     nbits=8 -> [0, 1, 2, ..., 256]\n",
    "    #     nbits=2 -> [0, 64, 128, 192, 256]\n",
    "    #     nbits=1 -> [0, 128, 256]\n",
    "    bins = np.arange(0, 256+1, 256 // 2**nbits)\n",
    "    \n",
    "    # Digitize returns the index of the bin each pixel belongs to. \n",
    "    # Note that the bin indices are 1-based, as the zero bin is defined\n",
    "    # as (-infinity, bins[0]), which is not relevant for our case (we do not\n",
    "    # have negative pixel values in uint8 images):\n",
    "    #      bin 0: (-infinity, bins[0])\n",
    "    #      bin 1: [bins[0], bins[1])\n",
    "    #      bin 2: [bins[1], bins[2])\n",
    "    #      ...\n",
    "    # Hence, we can subtract 1 to start indexing from 0.\n",
    "    result = np.digitize(image, bins) - 1\n",
    "    \n",
    "    # Scale bin indices back to intensity range (still stored as uint8)\n",
    "    result *= (256 // 2**nbits)\n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "\n",
    "def quantize(image, nbits):\n",
    "    \"\"\"\n",
    "    Method 2:\n",
    "    Quantize an image using bitwise operations for better performance.\n",
    "    Equivalent to Method 1, but faster.\n",
    "    \"\"\"\n",
    "    assert image.dtype == np.uint8, \"Image must be of type uint8\"\n",
    "    assert 1 <= nbits <= 8, \"Number of bits must be between 1 and 8\"\n",
    "    return ((image >> (8 - nbits)) << (8 - nbits)).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Differences between temporal and spatial sampling?\n",
    "# ...\n",
    "\n",
    "# Quantization: Reduce image bit depth\n",
    "nbits = [8, 6, 4, 3, 2, 1]\n",
    "fig, axes = plt.subplots(2, len(nbits) // 2, figsize=(9, 5))\n",
    "for i, nbit in enumerate(nbits):\n",
    "    gray_quantized = ...\n",
    "    ...\n",
    "\n",
    "# Spatial subsampling: Reduce resolution\n",
    "fig, axes = plt.subplots(2, len(nbits) // 2, figsize=(9, 5))\n",
    "gray_sub = gray.copy()\n",
    "for i in range(1,6):\n",
    "    gray_sub = ...\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise3'></a>\n",
    "\n",
    "## **&#9734;  Exercise 3 – Color representation**\n",
    "\n",
    "Gray is dull! Let’s add some color in this next exercise! We again load the same image as before, but this time in color. Let's give it a try!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the color image\n",
    "color = cv.imread(\"../data/images/kingfisher.jpg\")\n",
    "\n",
    "# We use here a convenience function to display the image. \n",
    "# It embeds the image directly in the notebook.\n",
    "isp.display_image(color)\n",
    "\n",
    "# Print image dimensions: height, width, and number of color channels\n",
    "print(\"Image shape (height, width, channels):\", color.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is indeed a color image – it's now represented using three channels. But the colors in the image look strange. This is because OpenCV loads images in **BGR format**, while Matplotlib expects **RGB format**. To correct this, you can simply reorder the channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_bgr = color.copy()\n",
    "color_rgb = color_bgr[..., ::-1]\n",
    "isp.display_image(color_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions**:\n",
    "* Examine the image's shape and depth.\n",
    "* Print the pixel value at position (100, 100).\n",
    "* Crop the image to the region [125:300, 300:600] and display the result.\n",
    "* Convert the image to an 8-bit grayscale image using at least two different methods (hint: refer to the lecture slides).\n",
    "* Use OpenCV’s built-in function [`cv.cvtColor(image, cv.COLOR_RGB2GRAY)`](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html) for grayscale conversion.\n",
    "* Optional: Apply the `quantize()` function from the previous exercise to quantize color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "print(\"Image shape (height, width, channels):\", ...)\n",
    "print(\"Data type:                            \", ...)\n",
    "print(\"Pixel value at [100, 100]:            \", ...)\n",
    "print()\n",
    "\n",
    "# Crop the image\n",
    "color_cropped = ...\n",
    "isp.display_image(color_cropped, scale=2.0)\n",
    "\n",
    "# Convert to grayscale:\n",
    "gray_method1 = ...\n",
    "gray_method2 = ...\n",
    "gray_opencv = ...\n",
    "isp.show_image_grid((color_rgb, gray_method1, gray_method2, gray_opencv),\n",
    "                    titles=(\"Original\", \"Method 1\", \"Method 2\", \"OpenCV\"))\n",
    "\n",
    "# Convert to n-bit image\n",
    "nbits = [8, 6, 4, 3, 2, 1]\n",
    "fig, axes = plt.subplots(2, len(nbits)//2, figsize=(12, 6))\n",
    "for i, nbit in enumerate(nbits):\n",
    "    color_quantized = ...\n",
    "    axes[i//3, i%3].imshow(color_quantized, cmap=\"gray\")\n",
    "    axes[i//3, i%3].set_title(f\"{nbit}-bit (2^{3*nbit} colors)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Digression: Pillow, color bands an dithering**\n",
    "\n",
    "\n",
    "[Pillow (PIL)](https://pillow.readthedocs.io/en/stable/) is a powerful and user-friendly alternative to OpenCV for image processing. It offers a simpler API and includes several advanced features. In this course, we will use Pillow here and there for tasks that are more easily or effectively handled with its tools.\n",
    "\n",
    "One such feature is its built-in color quantization method: [`PIL.Image.quantize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.quantize). It allows us to easily reduce the number of colors in an image, and optionally apply dithering to improve visual quality.\n",
    "\n",
    "**Dithering** is a technique that reduces visible quantization artifacts (such as color banding) by intentionally adding structured noise to the image. This noise is often correlated with the quantization error, which helps spread the error more evenly across the image. The result is a smoother, more natural-looking image.\n",
    "\n",
    "Below, we compare quantized images with and without dithering. Notice the visible color bands in the non-dithered version. The dithered image appears more natural, not only due to the dithering itself, but also because Pillow uses a more sophisticated quantization algorithm (e.g. k-means clustering), as opposed to the simple binning method we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image (represented as NumPy array) to a Pillow image. Easy!\n",
    "color_pil = PIL.Image.fromarray(color_rgb)\n",
    "\n",
    "#############################\n",
    "# Quantize WITHOUT dithering\n",
    "#############################\n",
    "ncolors = [256, 128, 64, 32, 16, 8]\n",
    "images = []\n",
    "for i, ncols in enumerate(ncolors):\n",
    "    img = color_pil.quantize(colors=ncols,\n",
    "                             method=PIL.Image.Quantize.MAXCOVERAGE,\n",
    "                             kmeans=1, )\n",
    "    images.append(img.convert(\"RGB\"))\n",
    "\n",
    "# Display results. Use a convenience function to show the images in a grid.\n",
    "isp.show_image_grid(images, titles=[f\"{nc} colors\" for nc in ncolors],\n",
    "                    figsize=(9, 6), ncols=3,\n",
    "                    header=\"Quantize WITHOUT dithering\")\n",
    "\n",
    "\n",
    "#############################\n",
    "# Quantize WITH dithering\n",
    "#############################\n",
    "\n",
    "# Dithering only works with paletted images, where each pixel holds an\n",
    "# index into a limited color palette. Pillow can generate a palette \n",
    "# adaptively or use the fixed \"web-safe\" palette of 256 colors.\n",
    "ncolors = [256, 128, 64, 32, 16, 8]\n",
    "images = []\n",
    "for i, ncols in enumerate(ncolors):\n",
    "    # Convert to a paletted image.\n",
    "    img = color_pil.convert(\"P\", dither=None, palette=PIL.Image.Palette.ADAPTIVE)\n",
    "    #img = color_pil.convert(\"P\", dither=None, palette=PIL.Image.Palette.WEB)\n",
    "    \n",
    "    # Apply quantization with Floyd–Steinberg dithering\n",
    "    img = img.quantize(colors=ncols, \n",
    "                       method=PIL.Image.Quantize.MAXCOVERAGE, \n",
    "                       kmeans=1, \n",
    "                       dither=PIL.Image.Dither.FLOYDSTEINBERG)\n",
    "    images.append(img.convert(\"RGB\"))\n",
    "\n",
    "isp.show_image_grid(images, titles=[f\"{nc} colors\" for nc in ncolors],\n",
    "                    figsize = (9, 6),\n",
    "                    header=\"Quantize WITH dithering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **&#9734;&#9734; More on color spaces**\n",
    "\n",
    "**Comment 1**: Some color spaces are more perceptually uniform than RGB. For example, the [CIE Lab](https://en.wikipedia.org/wiki/CIELAB_color_space) color space is specifically designed so that small changes in its values correspond to similarly small changes in perceived color. In contrast, in the RGB space, Euclidean distances between colors do not reliably reflect how different those colors appear to the human eye. Nevertheless, for display purposes, we often need to convert images back to RGB.\n",
    "\n",
    "\n",
    "**Comment 2**: Several color spaces separate intensity (or *luma*) from color (or *chroma*) information. This distinction is useful because the human visual system is much more sensitive to variations in brightness than to changes in color. Examples of such color spaces include [Y'UV](https://en.wikipedia.org/wiki/Y%E2%80%B2UV), [YCbCr](https://en.wikipedia.org/wiki/YCbCr), [HSL (or HSV)](https://en.wikipedia.org/wiki/HSL_and_HSV), and [CIE Lab](https://en.wikipedia.org/wiki/CIELAB_color_space). In many image processing applications, it is therefore common to quantize the intensity channel with more bits than the chroma channels.\n",
    "\n",
    "In the following, we will construct a color wheel using one of these color spaces, ensuring that all colors have a uniform intensity level (uniform *luma* L). Since the image will eventually be displayed in RGB, we will convert the color wheel back to the RGB space for visualization.\n",
    "\n",
    "Now, what should we expect if we convert this RGB color wheel to a grayscale image? Since the intensity (L) values were uniform in the Lab space, the grayscale version should appear relatively uniform as well. Let's check if that holds true. Any slight variations in gray levels may be due to inaccuracies or approximations in the color conversion functions between Lab and RGB.\n",
    "\n",
    "For information on color space conversions, see [here](https://docs.opencv.org/4.x/de/d25/imgproc_color_conversions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_color_wheel1(size=1000, rel_width=0.15, intensity=128):\n",
    "#     \"\"\"\n",
    "#     Create a color wheel with uniform intensity levels. (Slow version.)\n",
    "#     \"\"\"\n",
    "#     # Create a color wheel with uniform intensity levels\n",
    "#     img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "#     width = rel_width*size\n",
    "#     L = intensity\n",
    "#     for i in range(size):\n",
    "#         for j in range(size):\n",
    "#             # Compute the angle and radius\n",
    "#             x = i - size // 2\n",
    "#             y = j - size // 2\n",
    "#             angle = np.arctan2(y, x)\n",
    "#             radius = np.sqrt(x**2 + y**2)\n",
    "#             # Compute the chroma values (between 0 and 255)\n",
    "#             a = (np.cos(angle)+1) * 128\n",
    "#             b = (np.sin(angle)+1) * 128\n",
    "#             # Fix some rounding error by offsetting a and b\n",
    "#             a = a - 0.5\n",
    "#             b = b - 0.5\n",
    "#             # Convert to uint8\n",
    "#             a = a.astype(np.uint8)\n",
    "#             b = b.astype(np.uint8)\n",
    "#             # On circle?\n",
    "#             mask = (radius > size//2-width) & (radius < size//2)\n",
    "#             img[i, j] = [L, a, b] if mask else [0, 128, 128]\n",
    "#     return img\n",
    "\n",
    "def create_color_wheel(size=1000, rel_width=0.15, intensity=128):\n",
    "    \"\"\"\n",
    "    This version is much faster than the previous one.\n",
    "    \"\"\"\n",
    "    L = intensity\n",
    "    width = rel_width*size\n",
    "    J, I = np.meshgrid(np.arange(size), np.arange(size))\n",
    "    X = I - size // 2\n",
    "    Y = J - size // 2\n",
    "    angle = np.arctan2(Y, X)\n",
    "    radius = np.sqrt(X**2 + Y**2)\n",
    "    L = np.ones_like(X, dtype=np.uint8) * L\n",
    "    a = ((np.cos(angle)+1) * 128 - 0.5).astype(np.uint8)\n",
    "    b = ((np.sin(angle)+1) * 128 - 0.5).astype(np.uint8)\n",
    "    img = np.stack((L, a, b), axis=-1)\n",
    "    mask = (radius > size//2-width) & (radius < size//2)\n",
    "    # Black in LAB space\n",
    "    img[~mask] = [0, 128, 128]\n",
    "    return img\n",
    "\n",
    "\n",
    "# Create the color wheel. The first channel is the luminance, the other two\n",
    "# channels define the chrominance.\n",
    "target_intensity = 128\n",
    "img_lxx = create_color_wheel(size=1000, rel_width=0.15, \n",
    "                             intensity=target_intensity)\n",
    "\n",
    "# Convert to RGB (for display)\n",
    "# All of the following color spaces have in the first channel the luminance\n",
    "# and in the other two channels the chrominance. We can construct the \n",
    "# color wheel in the same way for all of them.\n",
    "#   YCrCb, Yuv, Luv, Lab\n",
    "\n",
    "images = []\n",
    "titles = []\n",
    "\n",
    "print(\"Target: Intensity:\", target_intensity)\n",
    "\n",
    "img_rgb = cv.cvtColor(img_lxx, cv.COLOR_YUV2RGB)\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "images.append(img_rgb)\n",
    "images.append(img_gray)\n",
    "titles.append(\"YUV color circle\")\n",
    "titles.append(\"YUV intensity\")\n",
    "print(\"YUV:    Gray level range\", img_gray[img_gray>0].min(), img_gray.max())\n",
    "\n",
    "img_rgb = cv.cvtColor(img_lxx, cv.COLOR_YCrCb2RGB)\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "images.append(img_rgb)\n",
    "images.append(img_gray)\n",
    "titles.append(\"YCrCb color circle\")\n",
    "titles.append(\"YCrCb intensity\")\n",
    "print(\"YCrCb:  Gray level range\", img_gray[img_gray>0].min(), img_gray.max())\n",
    "\n",
    "img_rgb = cv.cvtColor(img_lxx, cv.COLOR_LUV2RGB)\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "images.append(img_rgb)\n",
    "images.append(img_gray)\n",
    "titles.append(\"LUV color circle\")\n",
    "titles.append(\"LUV intensity\")\n",
    "print(\"LUV:    Gray level range\", img_gray[img_gray>0].min(), img_gray.max())\n",
    "\n",
    "img_rgb = cv.cvtColor(img_lxx, cv.COLOR_LAB2RGB)\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "images.append(img_rgb)\n",
    "images.append(img_gray)\n",
    "titles.append(\"LAB color circle\")\n",
    "titles.append(\"LAB intensity\")\n",
    "print(\"LAB:    Gray level range\", img_gray[img_gray>0].min(), img_gray.max())\n",
    "\n",
    "isp.show_image_grid(images, titles=titles,\n",
    "                    figsize=(6, 12), ncols=2,\n",
    "                    suppress_info=True,\n",
    "                    header=\"Color wheel in different color spaces\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise4'></a>\n",
    "\n",
    "## **&#9734;  Exercise 4 – Construct an image**\n",
    "\n",
    "In this exercise, you will create an image from scratch. Your task is to generate a checkerboard pattern consisting of 8×8 squares, where each square measures 32×32 pixels. The squares should alternate between two colors of your choice (e.g., red and green or black and white). The final image should be 256×256 pixels in total.\n",
    "\n",
    "Complete the function template below to implement your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "def create_checkerboard(square_size=32, checker_size=8, color1=[0,0,0], color2=[255,255,255]):\n",
    "    \"\"\"Create a checkerboard image with the specified square size and checker size.\"\"\"\n",
    "    image_size = square_size * checker_size\n",
    "    # Complete this function\n",
    "    iamge = ...\n",
    "    return image\n",
    "    \n",
    "checker = create_checkerboard(square_size=32, \n",
    "                              checker_size=8, \n",
    "                              color1=[255,188,188], \n",
    "                              color2=[255,255,188]) \n",
    "isp.display_image(checker, scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise5'></a>\n",
    "\n",
    "## **&#9734;  Exercise 5 – Gamma correction**\n",
    "\n",
    "Gamma correction is a non-linear operation used to adjust the brightness of an image. It compensates for the non-linear response of the human eye to light intensity, as well as the non-linear behavior of certain display devices.\n",
    "\n",
    "The gamma correction formula is:\n",
    "\n",
    "$$I_{out} = I_{in} ^ {\\;\\gamma}$$\n",
    "\n",
    "Here, $\\gamma > 0$ is the correction factor:\n",
    "* For $\\gamma=1$, the image remains unchanged.\n",
    "* For $\\gamma>1$, the image appears darker.\n",
    "* For $\\gamma<1$, the image appears brighter.\n",
    "\n",
    "\n",
    "**Hint**: Gamma correction should be applied to images in float format, with pixel values normalized to the range [0, 1]. This means you will need to convert the image's data type and scale the values accordingly. (Note: Matplotlib can display floating-point images just like images with integer pixel values.)\n",
    "\n",
    "\n",
    "### **Instruction:**\n",
    "Complete the function template below to apply gamma correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Solution\n",
    "def gamma_correction(image, gamma):\n",
    "    \"\"\"Apply gamma correction to an image with the specified gamma value.\"\"\"\n",
    "    # Conver to float if the image is in uint8 format\n",
    "    image = ...\n",
    "    # Apply the gamma correction\n",
    "    corrected = ...\n",
    "    return corrected\n",
    "\n",
    "# Test the function.\n",
    "image = color_rgb.copy()\n",
    "gamma = 1.5\n",
    "corrected = gamma_correction(image, gamma)\n",
    "isp.show_image_grid((image, corrected), titles=(\"Original\", f\"Gamma={gamma}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise6'></a>\n",
    "\n",
    "## **&#9734;  Exercise 6 – Color and intensity histograms**\n",
    "\n",
    "### **Instructions:**\n",
    "* Carefully read the following section\n",
    "* Make sure you understand the concepts of intensity and color histograms\n",
    "* Familiarize yourself with the types of color transformations explored here\n",
    "\n",
    "<br>\n",
    "\n",
    "The *distribution of intensity* values in an image (or a specific channel) characterizes an image. We can use image *histogram* to show how many pixels fall into each intensity level. Histograms are useful to analyze an image's contrast, brightness, and dynamic range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized histogram and cumulative distribution function (CDF)\n",
    "hist, bins = np.histogram(gray.flatten(), bins=256, range=[0,256], density=True)\n",
    "cdf = hist.cumsum()\n",
    "hist /= hist.max()  # Normalize histogram for display purposes\n",
    "\n",
    "# Visualize the histogram and CDF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(gray, cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].plot(hist, label=\"Histogram (normalized)\")\n",
    "axes[1].plot(cdf, label=\"CDF\")\n",
    "axes[1].set_xlabel(\"Pixel value\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].grid(axis=\"y\")\n",
    "axes[1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this input image, we observe that most pixel values are concentrated between 120 and 180, with very few pixels below 100. This distribution indicates that the image has a limited dynamic range in terms of brightness levels.\n",
    "\n",
    "The histogram plot above also shows the cumulative distribution function (CDF) of the pixel values. The CDF represents the fraction of pixels with intensity values less than or equal to a given value. For example, a CDF value of less than 0.1 at intensity 100 means that fewer than 10% of the pixels have values below 100.\n",
    "\n",
    "\n",
    "Such a distribution typically corresponds to a low-contrast image, where pixel values are clustered within a narrow range. To **enhance contrast**, we can apply contrast stretching, which remaps the pixel values to span a broader range. One common technique for this is **histogram equalization** – a method that improves image contrast by redistributing pixel values to achieve a more uniform histogram. The aim is to spread out the intensities so that all values occur more evenly, enhancing the visual quality of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply histogram equalization\n",
    "gray = cv.imread(\"../data/images/kingfisher-gray.jpg\", cv.IMREAD_GRAYSCALE)\n",
    "gray_equalized = cv.equalizeHist(gray)\n",
    "hist_equalized, bins = np.histogram(gray_equalized.flatten(), \n",
    "                                    bins=256, range=[0,256], \n",
    "                                    density=True)\n",
    "cdf_equalized = hist_equalized.cumsum()\n",
    "hist_equalized /= hist_equalized.max()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(gray_equalized, cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].plot(hist_equalized, label=\"Histogram (normalized)\")\n",
    "axes[1].plot(cdf_equalized, label=\"CDF\")\n",
    "axes[1].set_xlabel(\"Pixel value\")\n",
    "axes[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equalized image shows a more uniform histogram, meaning the pixel values are more evenly distributed across the intensity range. The CDF appears now linear, which further indicates an improvement in image contrast. However, histogram equalization can also amplify noise or unwanted artifacts. For example, quantization bands that were subtle in the original image may become more pronounced after equalization.\n",
    "\n",
    "For **color images**, we can apply histogram equalization to each channel independently. However, this can distort colors. A better approach is to convert the image to a color space that separates brightness from color information—such as YUV or HSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to visualize the histogram of all 3 channels in the image\n",
    "def visualize_histogram(image, title=\"Histogram\", channel_names=\"RGB\"):\n",
    "\n",
    "    def visualize_channel(img, title, ax, color):\n",
    "        hist, bins = np.histogram(img.flatten(), bins=256, range=[0,256], density=True)\n",
    "        cdf = hist.cumsum()\n",
    "        hist /= hist.max()\n",
    "        ax.plot(hist, label=\"Histogram (normalized)\", color=color)\n",
    "        ax.plot(cdf, label=\"CDF\", color=\"k\", linestyle=\":\")\n",
    "        if False:\n",
    "            ax.set_xlabel(\"Pixel value\")\n",
    "            ax.legend()\n",
    "        ax.set_title(title)\n",
    "\n",
    "    \"\"\"Visualize the histogram of an image.\"\"\"\n",
    "    if image.ndim == 1:\n",
    "        return visualize_channel(image, title, plt.gca())\n",
    "    else:\n",
    "        nchannels = image.shape[-1]\n",
    "        fig, axes = plt.subplots(1, nchannels+1, figsize=(9, 2))\n",
    "        axes[0].imshow(image, cmap=\"gray\" if nchannels == 1 else None)\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[0].set_title(title)\n",
    "        axes[0].set_anchor(\"N\")\n",
    "        titles = [\"%s channel\" % name for name in channel_names]\n",
    "        for i in range(image.shape[-1]):\n",
    "            visualize_channel(image[..., i], \n",
    "                              title=titles[i],\n",
    "                              ax=axes[i+1], color=isp.PALETTE_RGB[i])\n",
    "    #fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Convert the image to float\n",
    "color_equalized = np.stack([cv.equalizeHist(color_bgr[...,i]) for i in range(3)], axis=2)\n",
    "visualize_histogram(color_rgb, title=\"Original\")\n",
    "visualize_histogram(color_equalized, title=\"RGB equalized\")\n",
    "# Better, convert to HSL color space and apply histogram equalization \n",
    "# to the L (luminance) channel only!\n",
    "color_hls = cv.cvtColor(color_rgb, cv.COLOR_RGB2HLS)\n",
    "color_hls[..., 1] = cv.equalizeHist(color_hls[..., 1])\n",
    "color_equalized = cv.cvtColor(color_hls, cv.COLOR_HLS2RGB)\n",
    "visualize_histogram(color_equalized, \n",
    "                    title=\"HLS equalized\",\n",
    "                    channel_names=[\"H\", \"L*\", \"S\"])\n",
    "# Also the YUV color space can be used\n",
    "color_hls = cv.cvtColor(color_rgb, cv.COLOR_RGB2YUV)\n",
    "color_hls[..., 0] = cv.equalizeHist(color_hls[..., 0])\n",
    "color_equalized = cv.cvtColor(color_hls, cv.COLOR_YUV2RGB)\n",
    "visualize_histogram(color_equalized, \n",
    "                    title=\"YUV equalized\", \n",
    "                    channel_names=[\"Y*\", \"U\", \"V\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Histogram equalization can also be applied to color images. However, applying it independently to each RGB channel often leads to unnatural color distortions. A better approach is to apply the method to a single channel that represents the image’s luminance or intensity. This can be done by converting the image to a color space such as HLS or YUV, and applying equalization to the luminance channel only. This typically produces more visually appealing results.\n",
    "\n",
    "Overall, histogram equalization is a simple and effective technique for enhancing image contrast.\n",
    "\n",
    "Another related method is **histogram matching** (also known as histogram specification). Instead of flattening the histogram, it adjusts the contrast of an image by matching its histogram to that of a reference image. In Python, this can be done using the [`skimage.exposure.match_histograms()`](https://scikit-image.org/docs/stable/api/skimage.exposure.html=) function from the scikit-image library. It takes an input image and a reference image, and returns a version of the input image whose histogram closely resembles that of the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skimage import exposure\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "color1 = color_rgb.copy()\n",
    "color2 = cv.imread(\"../data/images/veggies.jpg\")\n",
    "color2 = cv.cvtColor(color2, cv.COLOR_BGR2RGB)\n",
    "color3 = cv.imread(\"../data/images/flowers.jpg\")\n",
    "color3 = cv.cvtColor(color3, cv.COLOR_BGR2RGB)\n",
    "\n",
    "matched12 = match_histograms(color1, color2, channel_axis=-1)\n",
    "matched23 = match_histograms(color1, color3, channel_axis=-1)\n",
    "\n",
    "isp.show_image_grid((color1, color2, matched12), \n",
    "                    titles=(\"Input\", \"Reference\", \"Matched\"),\n",
    "                    suppress_info=True,\n",
    "                    figsize=(9, 6))\n",
    "isp.show_image_grid((color1, color3, matched23), \n",
    "                    titles=(\"Input\", \"Reference\", \"Matched\"),\n",
    "                    suppress_info=True,\n",
    "                    figsize=(9, 6))\n",
    "\n",
    "visualize_histogram(color_rgb, title=\"Input\")\n",
    "visualize_histogram(color3, title=\"Input\")\n",
    "visualize_histogram(matched23, title=\"Matched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the CDFs of the matched image closely resemble those of the reference image – demonstrating the effectiveness of histogram matching.\n",
    "\n",
    "Histograms are also a useful tool for understanding the effects of various color and **intensity adjustments** on an image. Below, we demonstrate how different operations impact the image and its histogram:\n",
    "\n",
    "* No operation (copy image)\n",
    "* Inversion\n",
    "* Brightness increase / decrease\n",
    "* Contrast stretching / compression\n",
    "* Clipping (limiting values to a specified [min, max] range)\n",
    "* Binarization (thresholding)\n",
    "* Ggamma correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "lookup_tables = []\n",
    "image = gray.copy()\n",
    "results = []\n",
    "normalize = False  # Do not normalize the values for display!\n",
    "\n",
    "labels.append(\"Identity\")\n",
    "lookup_table = np.arange(256)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Inverted\")\n",
    "lookup_table = 255 - np.arange(256)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Increase brightness\")\n",
    "lookup_table = np.clip(np.arange(256) + 50, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Decrease brightness\")\n",
    "lookup_table = np.clip(np.arange(256) - 50, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Contrast stretch\")\n",
    "# Lookup such that mean remains equal, but the range is stretched\n",
    "# (The peak of the histogram is at 163)\n",
    "center = 163\n",
    "lookup_table = np.clip((np.arange(256) - center) * 3 + center, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Contrast squeeze\")\n",
    "lookup_table = np.clip((np.arange(256) - center) / 6 + center, 0, 255)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Clipping\")\n",
    "lookup_table = np.clip(np.arange(256), 135, 175)\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Threshold\")\n",
    "lookup_table = (np.arange(256) > 128) * 255\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Gamma=0.5\")\n",
    "lookup_table = (np.arange(256) / 255) ** 0.5 * 255\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "labels.append(\"Gamma=2.0\")\n",
    "lookup_table = (np.arange(256) / 255) ** 2.0 * 255\n",
    "lookup_tables.append(lookup_table)\n",
    "\n",
    "\n",
    "# Visualize the results\n",
    "for label, lookup in zip(labels, lookup_tables):\n",
    "    result = lookup[image]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    # Display the result\n",
    "    if not normalize:\n",
    "        vmin, vmax = 0, 255\n",
    "    else:\n",
    "        vmin, vmax = result.min(), result.max()\n",
    "    axes[0].imshow(result, cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title(label, fontweight=\"bold\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_anchor(\"N\")\n",
    "    # Display lookup table\n",
    "    axes[1].plot(lookup)\n",
    "    axes[1].set_title(\"Lookup table\")\n",
    "    axes[1].set_xlabel(\"Input value\")\n",
    "    axes[1].set_ylabel(\"Output value\")\n",
    "    axes[1].set_xlim([0-5, 255+5])\n",
    "    axes[1].set_ylim([0-5, 255+5])\n",
    "    axes[1].set_aspect(\"equal\")\n",
    "    axes[1].grid(axis=\"y\")\n",
    "    # Display the histograms (before, after)\n",
    "    hist, bins = np.histogram(image.flatten(), bins=256, range=[0,256], density=True)\n",
    "    axes[2].plot(hist, label=\"Before\")\n",
    "    hist, bins = np.histogram(result.flatten(), bins=256, range=[0,256], density=True)\n",
    "    axes[2].plot(hist, label=\"After\")\n",
    "    axes[2].set_title(\"Histogram\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(axis=\"y\")\n",
    "    fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-isp-fs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
