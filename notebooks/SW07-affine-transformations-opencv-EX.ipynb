{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Geometric image transformations (with OpenCV)**\n",
    "\n",
    "<div style=\"color:#777777;margin-top: -15px;\">\n",
    "<b>Author</b>: Norman Juchler |\n",
    "<b>Course</b>: ADLS ISP |\n",
    "<b>Version</b>: v1.2 <br><br>\n",
    "<!-- Date: 03.04.2025 -->\n",
    "<!-- Comments: Fully refactored. -->\n",
    "<!-- TODO: Demonstrate the forward and backward transformation: iteration over the input image pixels, vs. iteration over the output image pixels... INVERSE... -->\n",
    "</div>\n",
    "\n",
    "It is a common task to move, rotate, scale or otherwise transform an image. In this tutorial, you will learn about different ways how to perform various geometric transformations programmatically in Python and OpenCV.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "\n",
    "## **☆ Exercise 1: Manually transform an image**\n",
    "\n",
    "Use an image editing program to solve the following task. Open image A (missing.png) and image B (piece-scaled-transparent.png), and try to align the missing piece so that it fits perfectly into the gap.\n",
    "\n",
    "### **Instructions**\n",
    "\n",
    "* Open your favorite image editing program\n",
    "* Load image A\n",
    "* Insert image B\n",
    "* Translate, rotate and scale image B until it aligns with the gap\n",
    "\n",
    "Here is a list of possible editors you could use:\n",
    "* Adobe Photoshop\n",
    "* Adobe Illustrator\n",
    "* GIMP\n",
    "* Paint.NET (Win)\n",
    "* Preview (MacOS)\n",
    "* Microsoft Photo Editor (Win)\n",
    "* Microsoft PowerPoint\n",
    "* ...\n",
    "\n",
    "(Let your tutor know if your favorite image editor is missing from the list!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "Before we start, let's import some packages. The package `isp` provides some helper functions to display images easily in this Jupyter notebook. (You are welcome to look into these functions if you are curious, but a deep understanding is not required.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "# Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable this line if you want to use the interactive widgets\n",
    "# It requires the ipympl package to be installed.\n",
    "#%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import isp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's load the images and take a look at them. Note that they include an alpha channel (transparency). In this tutorial, we will preserve this extra channel, although all the operations shown below would work just as well with standard RGB images.\n",
    "\n",
    "\n",
    "The **alpha channel** is an additional channel in an image that represents transparency. While an RGB image uses three channels (red, green, and blue) to define color, an RGBA image adds a fourth channel – alpha – which indicates how opaque or transparent each pixel is.\n",
    "\n",
    "* A value of 0 means the pixel is fully transparent\n",
    "* A value of 255 (or 1.0 in normalized form) means the pixel is fully opaque\n",
    "\n",
    "This is especially useful when layering or blending images, such as in graphics, UI design, or computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image (flag IMREAD_UNCHANGED preserves the alpha channel)\n",
    "imageA = cv.imread(filename=\"../data/images/water2/missing.jpg\", \n",
    "                   flags=cv.IMREAD_COLOR)\n",
    "imageB = cv.imread(filename=\"../data/images/water2/piece-scaled.jpg\", \n",
    "                   flags=cv.IMREAD_COLOR)\n",
    "\n",
    "# Convert the image from BGRA to RGBA\n",
    "imageA = cv.cvtColor(imageA, cv.COLOR_BGRA2RGBA)\n",
    "imageB = cv.cvtColor(imageB, cv.COLOR_BGRA2RGBA)\n",
    "\n",
    "# Display the images side-by-side\n",
    "titleA = f\"Image A\"\n",
    "titleB = f\"Image B\"\n",
    "isp.show_image_pair(imageA, imageB, title1=titleA, title2=titleB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise2'></a>\n",
    "\n",
    "## **☆ Exercise 2: Resize an image**\n",
    "\n",
    "Try scaling imageB by different factors and observe the results. You can use [`cv.resize()`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d) for this task.\n",
    "\n",
    "```python\n",
    "# Resize image to specific dimensions:\n",
    "scaled = cv.resize(image, (new_width, new_height))\n",
    "# Resize using scaling factors:\n",
    "scaled = cv.resize(image, None, fx=factor_x, fy=factor_y)\n",
    "```\n",
    "\n",
    "### **Instructions**\n",
    "* Trasnformation 1: Scale imageB to size (1000, 1800)\n",
    "* Transformation 2: Stretch imageB by factor 2 in x-direction, and squeeze it in y-direction by factor 0.5\n",
    "* Transformation 3: Scale imageB by factor 1.5, preserving the aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Task 1:\n",
    "scaled = cv.resize(...)\n",
    "isp.show_image_pair(imageB, scaled, title1=\"input\", title2=\"scaled\")\n",
    "\n",
    "# Task 2:\n",
    "...\n",
    "\n",
    "# Task 3:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Affine warping with transformation matrices**\n",
    "\n",
    "Note: To better understand this section, we recommend reading the primer on *Affine Transformations* available in our project repository.\n",
    "\n",
    "Affine transformations can be described using matrices. In **homogeneous coordinates**, the transformation matrix takes the following form:\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a_{00} & a_{01} & t_0 \\\\\n",
    "a_{10} & a_{11} & t_1 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Using this matrix, an affine transformation can be expressed as:\n",
    "\n",
    "$$y = A\\cdot x$$\n",
    "\n",
    "Here, the matrix $A$ encodes all transformation parameters. The vector $x$ represents the coordinates of a pixel (in homogeneous coordinates) in the input image, and $y$ is the corresponding position in the target image. \n",
    "\n",
    "\n",
    "Image scaling is one of the simplest transformations that can be represented using such a matrix $A$. Let's try it out in OpenCV! Note that OpenCV also operates with affine transformation matrices, but it **omits the last row** of $A$, since it is always constant for affine transformations: $[0, 0, 1]$. Therefore, OpenCV uses a simplified 2×3 matrix format:\n",
    "\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a_{00} & a_{01} & t_0 \\\\\n",
    "a_{10} & a_{11} & t_1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the scaling factors\n",
    "scale_x = 2.0  # scaling factor in x-direction\n",
    "scale_y = 0.5  # scaling factor in y-direction\n",
    "\n",
    "# Define the scaling matrix\n",
    "scaling_matrix = np.float32([[scale_x, 0, 0], [0, scale_y, 0]])\n",
    "\n",
    "# Display the matrix\n",
    "print(f\"Scaling matrix:\\n{scaling_matrix}\")\n",
    "\n",
    "# Apply the scaling\n",
    "scaled = cv.warpAffine(imageB, scaling_matrix, (1000,1000))\n",
    "cv.imwrite(\"scaled.png\", cv.cvtColor(scaled, cv.COLOR_RGBA2BGRA))\n",
    "cv.imwrite(\"imageB.png\", cv.cvtColor(imageB, cv.COLOR_RGBA2BGRA))\n",
    "\n",
    "# Display the result\n",
    "isp.show_image_pair(imageB, scaled, title1=\"input\", title2=\"scaled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise3'></a>\n",
    "\n",
    "## **☆ Exercise 3: Move an image**\n",
    "\n",
    "\n",
    "### **Instructions**\n",
    "\n",
    "* Translate `imageB` by 100 pixels in the x-direction and 250 pixel in the y-direction\n",
    "* Use the function [`cv::warpAffine()`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Set the parameters here\n",
    "tx = 100\n",
    "ty = 250\n",
    "\n",
    "# Create the translation matrix\n",
    "translation_matrix = ...\n",
    "\n",
    "# Apply the transformation\n",
    "imageT = ...\n",
    "\n",
    "# Display the original and translated images\n",
    "isp.show_image_chain(images=(imageB, imageT), \n",
    "                     titles=(\"input\", \"translated\"),\n",
    "                     shape=(1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise4'></a>\n",
    "\n",
    "## **☆ Exercise 4: Rotation with an arbitrary center**\n",
    "\n",
    "A rotation is defined by two parameters: a rotation angle $\\phi$ and a center of rotation $C$ (the hinge point).\n",
    "\n",
    "OpenCV provides the function [`cv.getRotationMatrix2D(center, angle, scale)`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326), which returns a 2×3 transformation matrix for rotating (and optionally scaling) an image around an arbitrary center point.\n",
    "\n",
    "The resulting *transformation matrix* $A$ encodes all the information required to perform the transformation. It can be passed directly to [`cv.warpAffine(image, matrix, shape)`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983), which applies the transformation and returns a new image with the specified `shape = (width, height)`.\n",
    "\n",
    "⚠️ **Note:** OpenCV expects angles in *degrees*, while `numpy` functions often expect *radians* unless stated otherwise!\n",
    "\n",
    "\n",
    "### **Instructions**:\n",
    "1. Rotate `imageA` 70° around the center of the image  \n",
    "2. Rotate `imageB` −30° around the center of the image  \n",
    "3. Rotate `imageA` 30° around the top-left corner (coordinates: (0, 0))  \n",
    "4. Rotate `imageB` 90° around the bottom-right corner (coordinates: (`width`, `height`))\n",
    "\n",
    "Start with a scaling factor of 1.0, then repeat each transformation with a scaling factor of 2.0.\n",
    "\n",
    "Explain your observations!\n",
    "- In which direction does the image rotate for positive angles?\n",
    "- Do you understand all the results? If not, try visualizing the hinge point and rotation direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "scale = 1.0\n",
    "\n",
    "# Task 1:\n",
    "angle = 70\n",
    "\n",
    "# Task 2:\n",
    "...\n",
    "\n",
    "# Task 3:\n",
    "...\n",
    "\n",
    "# Task 4:\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercise5'></a>\n",
    "\n",
    "## **☆☆ Exercise 5: Compound transformations**\n",
    "\n",
    "In the primer on affine transformations, we learned that multiple elementary transformations can be combined (concatenated) to form more complex transformations. The function [`cv.getRotationMatrix2D()`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html) returns such a compound transformation matrix.\n",
    "\n",
    "The goal of this exercise is to understand how the transformation matrix returned by `cv.getRotationMatrix2D()` is composed.\n",
    "\n",
    "### **Instructions:**\n",
    "1. Review Figure 2 in the primer on affine transformations\n",
    "2. Consider: Which elementary transformations might be combined in the matrix returned by `cv.getRotationMatrix2D()`?\n",
    "3. Try to construct the transformation matrix manually:\n",
    "   - Use `imageB` as input\n",
    "   - Choose the image center as the rotation center\n",
    "   - Apply a scaling factor of 2.0 (in both x and y directions)\n",
    "   - Set the rotation angle to +30°\n",
    "4. Reflect: Does the order of transformation steps (scaling, rotation, translation) matter? Explain your reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXERCISE    ###\n",
    "######################\n",
    "\n",
    "# Task 1 + 2:\n",
    "# ...\n",
    "\n",
    "# Task 3:\n",
    "\n",
    "# Create and show the reference transform:\n",
    "center = (imageB.shape[1]//2, imageB.shape[0]//2)\n",
    "scale = 2.0\n",
    "angle = 30\n",
    "\n",
    "A_cv = cv.getRotationMatrix2D(center, angle=angle, scale=scale)\n",
    "\n",
    "print(\"Rotation matrix OpenCV:\")\n",
    "print(\"-----------------------\")\n",
    "print(A_cv.round(3))\n",
    "print(\"\")\n",
    "\n",
    "# Construct the matrix by combining elementary transformations.\n",
    "# Hint: To multiply two matrices A and B, you can use the\n",
    "#       np.matmul(A, B), the operator @, or np.dot(A, B).\n",
    "...\n",
    "A_manual = ...\n",
    "\n",
    "print(\"Rotation matrix manual:\")\n",
    "print(\"-----------------------\")\n",
    "print(A_manual.round(3))\n",
    "\n",
    "# Verify visually\n",
    "rotated_cv = cv.warpAffine(imageB, A_cv, (1000, 1000))\n",
    "rotated_manual = cv.warpAffine(imageB, A_manual, (1000, 1000))\n",
    "isp.show_image_pair(rotated_cv, rotated_manual, title1=\"OpenCV\", title2=\"manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example: Compound transformations**\n",
    "\n",
    "This example demonstrates how to apply a sequence of transformations to an image. The following steps are performed:\n",
    "\n",
    "- Translate the image by (100, 200) pixels  \n",
    "- Rotate the image by 45° around its center\n",
    "- Scale the image by a factor of 1.5 in the x-direction and 0.8 in the y-direction  \n",
    "\n",
    "These transformations can either be applied step by step, or they can be combined into a single transformation matrix and applied in one step using `cv.warpAffine()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Translate the image\n",
    "#############################\n",
    "tx = 100  # translation in the x-axis\n",
    "ty = 200  # translation in the y-axis\n",
    "translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "translated_image = cv.warpAffine(imageB, translation_matrix, (1000, 1000))\n",
    "\n",
    "# Step 2: Rotate the image about center\n",
    "#######################################\n",
    "angle = 45  # rotation angle in degrees\n",
    "height, width = translated_image.shape[:2]\n",
    "center = (width // 2, height // 2)\n",
    "rotation_matrix = cv.getRotationMatrix2D(center, angle, 1.0)\n",
    "rotated_image = cv.warpAffine(translated_image, rotation_matrix, (width, height))\n",
    "\n",
    "# Step 3: Scale the image\n",
    "#########################\n",
    "scale_x = 1.5  # scaling factor in the x-axis\n",
    "scale_y = 0.8  # scaling factor in the y-axis\n",
    "scaling_matrix = np.float32([[scale_x, 0, 0], [0, scale_y, 0]])\n",
    "scaled_image = cv.warpAffine(rotated_image, scaling_matrix, \n",
    "                             (rotated_image.shape[1], rotated_image.shape[0]))\n",
    "result1 = scaled_image\n",
    "\n",
    "# Alternative: Combine the transformations into a single matrix\n",
    "###############################################################\n",
    "rotation_matrix = np.vstack((rotation_matrix, [0, 0, 1]))\n",
    "translation_matrix = np.vstack((translation_matrix, [0, 0, 1]))\n",
    "scaling_matrix = np.vstack((scaling_matrix, [0, 0, 1]))\n",
    "combined_matrix = scaling_matrix @ rotation_matrix @ translation_matrix\n",
    "combined_matrix = combined_matrix[:2, :]\n",
    "result2 = cv.warpAffine(imageB, combined_matrix, (1000, 1000))\n",
    "\n",
    "# Display the original and transformed images\n",
    "isp.show_image_chain((imageB, translated_image, rotated_image, scaled_image),\n",
    "                     titles=(\"input\", \"translated\", \"rotated\", \"scaled\"), \n",
    "                     shape=(1000, 1000))\n",
    "# Display the comparison of the two methods\n",
    "isp.show_image_pair(result1, result2, title1=\"step-by-step\", title2=\"combined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Perspective transformations**\n",
    "\n",
    "<a id='exercise6'></a>\n",
    "\n",
    "The code below demonstrates how to apply a perspective transformation. It reproduces the example on the affine transformation primer. \n",
    "\n",
    "The function [`cv.getPerspectiveTransform()`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae) takes two arrays that specify the colored rectangles in *physical* coordinates, not pixel coordinates: The first coordinate refers to the horizontal position, the second coordinate to the vertical position.\n",
    "\n",
    "Using `cv.getPerspectiveTransform()`, you can map any pair of quadrilaterals, as illustrated in the second example. This makes it ideal for correcting perspective distortions or simulating viewpoint changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(filename=\"../data/images/dog-face.jpg\", \n",
    "                  flags=cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Rectangle coordinates\n",
    "rect_in = np.array([[75, 132],\n",
    "                    [607, 152],\n",
    "                    [711, 796],\n",
    "                    [21, 805]], dtype=np.float32)\n",
    "rect_out1 = np.array([[50, 150],\n",
    "                      [650, 150],\n",
    "                      [650, 850],\n",
    "                      [50, 850]], dtype=np.float32)\n",
    "rect_out2 = np.array([[40, 180],\n",
    "                      [1280, 20],\n",
    "                      [450, 850],\n",
    "                      [150, 750]], dtype=np.float32)\n",
    "\n",
    "# Compute a perspective transform, example 1\n",
    "matrix = cv.getPerspectiveTransform(rect_in, rect_out1)\n",
    "image_new1 = cv.warpPerspective(image, matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Compute a perspective transform, example 1\n",
    "matrix = cv.getPerspectiveTransform(rect_in, rect_out2)\n",
    "image_new2 = cv.warpPerspective(image, matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 9))\n",
    "isp.show_image(image, title=\"Original Image\", ax=ax1);\n",
    "isp.show_image(image_new1, title=\"Rectified image\", ax=ax2);\n",
    "isp.show_image(image_new2, title=\"Distorted image\", ax=ax3);\n",
    "# Add patch polygons to the image\n",
    "ax1.add_patch(plt.Polygon(rect_in, fill=False, edgecolor=[0,0.8,0], lw=2))\n",
    "ax2.add_patch(plt.Polygon(rect_out1, fill=False, edgecolor=[0.8,0,0], lw=2))\n",
    "ax3.add_patch(plt.Polygon(rect_out2, fill=False, edgecolor=[0,0,0.8], lw=2));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-isp-fs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
