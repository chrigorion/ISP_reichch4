{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mask operations**\n",
    "\n",
    "<div style=\"color:#777777;margin-top: -15px;\">\n",
    "<b>Author</b>: Norman Juchler |\n",
    "<b>Course</b>: ADLS ISP |\n",
    "<b>Version</b>: v1.2 <br><br>\n",
    "<!-- Date: 06.05.2025 -->\n",
    "<!-- Comments: Entirely refactored -->\n",
    "</div>\n",
    "\n",
    "In this notebook, we explore various operations involving image masks, along with useful techniques that can be applied in different image processing contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "The usual preparations... The package `isp` provides some helper functions to easily render images in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "# Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Functionality related to this course\n",
    "sys.path.append(\"..\")\n",
    "import isp\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will work with the following images. The first image shows a hematology sample (a blood smear) containing red blood cells and one white blood cell. The second image is a mask that labels only the red blood cells. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default dataset (feel free to change)\n",
    "mask = cv.imread(\"../data/images/hematology-baso1-mask.png\", cv.IMREAD_GRAYSCALE)\n",
    "img = cv.imread(\"../data/images/hematology-baso1.jpg\", cv.IMREAD_COLOR)\n",
    "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "isp.show_image_chain([img, mask], titles=[\"Image\", \"Mask (red blood cells)\"], suppress_info=True, figsize=(9, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For educational purposes, we introduce a second mask that has been distorted with pepper noise and randomly placed holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distortion 1: Add Gaussian noise\n",
    "np.random.seed(2)\n",
    "mask_noisy = np.ones(mask.shape, dtype=np.uint8) * 255\n",
    "mask_noisy[np.random.rand(*mask_noisy.shape) < 0.1] = 0\n",
    "\n",
    "# Distortion 2: Add larger holes. Place the holes randomly, but at a \n",
    "# certain distance from the edges, so that the holes will not change the\n",
    "# shape of the mask.\n",
    "mask_central = cv.erode(mask, None, iterations=11)\n",
    "indices = np.argwhere(mask_central > 0)\n",
    "num_holes = 100\n",
    "selection = np.random.choice(len(indices), num_holes, replace=False)\n",
    "centers_holes = indices[selection]\n",
    "mask_holes = np.ones(mask.shape, dtype=np.uint8)*255\n",
    "for center in centers_holes:\n",
    "    radius = np.random.randint(3, 9)\n",
    "    # [::-1]: swap x and y coordinates\n",
    "    cv.circle(mask_holes, tuple(center[::-1]), radius, 0, -1)\n",
    "    \n",
    "# Combine the masks\n",
    "mask_distorted = cv.bitwise_and(mask_holes, mask)\n",
    "mask_distorted = cv.bitwise_and(mask_distorted, mask_noisy)\n",
    "\n",
    "isp.show_image_chain([mask, mask_distorted], \n",
    "                     titles=[\"Original mask\", \"Distorted mask\"], \n",
    "                     suppress_info=True, figsize=(9, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Connected components**\n",
    "\n",
    "Assume you have a binary segmentation mask containing multiple objects. The goal is to identify the number of distinct objects and determine their bounding boxes. This brings us to the concept of connected components.\n",
    "\n",
    "A **connected component** is a group of adjacent foreground pixels in a binary image that are connected based on a defined connectivity. Each connected component corresponds to a distinct object or region in the image.\n",
    "\n",
    "In this context, **connectivity** defines how pixels are considered neighbors, see Figure 1. With 4-connectivity, only directly adjacent pixels (up, down, left, right) are connected. With 8-connectivity, diagonal neighbors are also included. The choice of connectivity influences how objects are separated or grouped during labeling.\n",
    "\n",
    "<img src=\"../data/doc/connected-components-2d.png\" style=\"width:40%;\">\n",
    "\n",
    "**Figure 1**: Pixel connectivity in 2D relative to the central pixel.\n",
    "\n",
    "OpenCV provides a connected components algorithm that labels each connected region in a binary image. It returns a labeled image of the same size as the input, where each pixel is assigned a label indicating the connected component it belongs to. Background pixels are labeled with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connectivity: 4 or 8.\n",
    "#   - 4: two pixels are connected if they share an edge.\n",
    "#   - 8: two pixels are connected if they share an edge or a corner.\n",
    "labels = cv.connectedComponents(mask, connectivity=8)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this section demonstrates different ways to visualize the individual connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container for visualization\n",
    "results = {}\n",
    "results[\"Input\"] = mask\n",
    "\n",
    "# Visualization 1: \n",
    "# ################\n",
    "# Display the connected components in different colors.\n",
    "def colorize_labels(labels):\n",
    "    \"\"\"\n",
    "    Represent the data in HSV such that the connected components are colored\n",
    "    differently. Recall: HSV refers to hue, saturation, and value. Hue \n",
    "    represents an angle in the color wheel, saturation the intensity of the \n",
    "    color, and value the brightness. To represent black, we set value to zero.\n",
    "    \"\"\"\n",
    "    h = labels/labels.max()*128\n",
    "    s = np.ones_like(labels)*255\n",
    "    v = (labels > 0) * 255\n",
    "    labels_color = np.stack([h, s, v], axis=-1)\n",
    "    labels_color = cv.cvtColor(labels_color.astype(np.uint8),\n",
    "                               cv.COLOR_HSV2RGB)\n",
    "    return labels_color\n",
    "\n",
    "labels_color = colorize_labels(labels)\n",
    "results[\"Connected components\"] = labels_color\n",
    "\n",
    "# Visualization 2: \n",
    "# ################\n",
    "# Restrict the number of colors and shuffle them. \n",
    "# We can use a look-up table (LUT) for this purpose.\n",
    "def colorize_labels_random(labels):\n",
    "    h_vals = [1, 34, 55, 99]  \n",
    "    assert 0 not in h_vals  # Zero is reserved for the background\n",
    "    # For reproducability\n",
    "    np.random.seed(1)\n",
    "    # Sample n times with replacement from h_vals \n",
    "    lut = np.random.choice(h_vals, labels.max())\n",
    "    # Insert a zero at the beginning to represent the background\n",
    "    lut = np.insert(lut, 0, 0)\n",
    "    # Lookup values\n",
    "    labels_new = lut[labels]\n",
    "    labels_color = colorize_labels(labels_new)\n",
    "    return labels_color\n",
    "\n",
    "labels_color = colorize_labels_random(labels)\n",
    "results[\"Randomized colors\"] = labels_color\n",
    "\n",
    "isp.show_image_grid(results, figsize=(10, 10), suppress_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is powered by `cv.connectedComponents()`. Note that we can also use  `scipy.ndimage.label()` for this purpose. The latter is more flexible and can be used for n-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as si\n",
    "labels, n_labels = si.label(mask)\n",
    "\n",
    "# It's output is equivalent to cv.connectedComponents().\n",
    "ret = colorize_labels(labels)\n",
    "\n",
    "# Compare the results\n",
    "isp.show_image_chain([results[\"Connected components\"], ret],\n",
    "                     titles=[\"OpenCV: cv.connectedComponents()\", \n",
    "                             \"Scipy: scipy.ndimage.label()\"], \n",
    "                     suppress_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to compute various properties for each of the extracted components. Before doing so, let's introduce a few related concepts—such as contours and convex hulls—that will be useful for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contours**\n",
    "\n",
    "Contours are the boundaries of connected components in a binary image. They can be extracted directly using `cv.findContours()`.\n",
    "\n",
    "This function returns two values: a list of contours and hierarchy information (as a list). The hierarchy contains four indices for each contour: `[next, previous, first child, parent]`. Each value is an index referring to another contour; if no such relation exists, the value is -1.\n",
    "\n",
    "In the following examples, we will ignore the hierarchy information. However, it becomes useful when dealing with nested contours – such as objects that contain holes or are enclosed within other objects.\n",
    "\n",
    "The `mode` argument in `cv.findContours()` determines how contours are retrieved. The most common ones are:\n",
    "- `cv.RETR_EXTERNAL`: Retrieve only the outermost (external) contours.\n",
    "- `cv.RETR_LIST`: Retrieve all contours in a flat list, without hierarchical information.\n",
    "- `cv.RETR_CCOMP`: Retrieve all contours as a two-level hierarchy (external and holes).\n",
    "- `cv.RETR_TREE`: Retrieve all contours and reconstructs the full hierarchy as a tree.\n",
    "\n",
    "The `method` parameter controls how the contour points are approximated. The goal is to reduce the number of points while preserving the overall shape of the contour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv.findContours(mask, \n",
    "                              mode=cv.RETR_EXTERNAL, \n",
    "                              method=cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cv.drawContours()` function allows us to conveniently draw and fill contours. In the following examples, we demonstrate several ways to use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Input\"] = mask\n",
    "\n",
    "# Visualization 1: Simple contours\n",
    "# ################################\n",
    "img_outlines = np.zeros_like(img)\n",
    "color = [255, 255, 0]\n",
    "cv.drawContours(img_outlines, contours, \n",
    "                contourIdx=-1, \n",
    "                color=color, \n",
    "                thickness=2)\n",
    "results[\"Plain contours\"] = img_outlines\n",
    "\n",
    "# Visualization 2: Single contour (filled)\n",
    "# ########################################\n",
    "img_outlines = np.zeros_like(img)\n",
    "color = [50, 100, 255]\n",
    "contour_id = 42\n",
    "cv.drawContours(img_outlines, contours, \n",
    "                contourIdx=contour_id, \n",
    "                color=color, \n",
    "                thickness=-1)\n",
    "results[\"Single contour (filled)\"] = img_outlines\n",
    "\n",
    "# Visualization 3: Use colors\n",
    "# ###########################\n",
    "import matplotlib as mpl\n",
    "# Use colormaps from matplotlib\n",
    "# https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
    "cmap = mpl.colormaps[\"inferno\"]\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(contours))]\n",
    "# Convert colors from [0, 1] to [0, 255]\n",
    "colors = [[int(c*255) for c in color] for color in colors]\n",
    "img_outlines = img.copy()\n",
    "for i, contour in enumerate(contours):\n",
    "    cv.drawContours(img_outlines, \n",
    "                    contours, \n",
    "                    contourIdx=i, \n",
    "                    color=colors[i], \n",
    "                    thickness=3)\n",
    "results[\"Contours in color\"] = img_outlines\n",
    "\n",
    "# Visualization 4: Random colors\n",
    "# ##############################\n",
    "img_outlines = img.copy()\n",
    "contours_shuffled = list(contours)\n",
    "#np.random.shuffle(contours_shuffled)\n",
    "colors = [[50, 100, 255],\n",
    "          [255, 100, 50],\n",
    "          [50, 255, 100]]\n",
    "for i, contour in enumerate(contours_shuffled):\n",
    "    cv.drawContours(img_outlines, \n",
    "                    contours_shuffled, \n",
    "                    contourIdx=i, \n",
    "                    color=colors[i % len(colors)], \n",
    "                    thickness=3)\n",
    "results[\"Randomized colors\"] = img_outlines\n",
    "\n",
    "# Visualization 5: Colorize based on size\n",
    "# #######################################\n",
    "sizes = np.array([cv.contourArea(contour) for contour in contours])\n",
    "sizes = sizes/sizes.max()  # Normalized sizes\n",
    "img_outlines = img.copy()\n",
    "# Use the following line if you want to sort the contours by size\n",
    "# contours_sorted = sorted(contours, key=cv.contourArea)\n",
    "\n",
    "# Use the colormap \"hsv\". It contains a transition red-yellow-green.\n",
    "# cmap is a function that maps a scalar between 0 and 1 to a color.\n",
    "# To sample only colors in the red-yellow-green range, we can use only\n",
    "# the first 30% of the colormap. \n",
    "cmap = mpl.colormaps[\"hsv\"]\n",
    "colors = [cmap(s*0.3) for s in sizes]\n",
    "colors = [[int(c*255) for c in color] for color in colors]\n",
    "\n",
    "for i, contour in enumerate(contours):\n",
    "    cv.drawContours(img_outlines, \n",
    "                    contours, \n",
    "                    contourIdx=i, \n",
    "                    color=colors[i % len(colors)], \n",
    "                    thickness=-1)  # Filled\n",
    "results[\"Colors sorted by size\"] = img_outlines\n",
    "\n",
    "# Visualization 6: Bounding boxes and centroids\n",
    "# #############################################\n",
    "def draw_bounding_boxes(img, contours):\n",
    "    assert img.ndim == 3\n",
    "    img = img.copy()\n",
    "    cmap = mpl.colormaps[\"hsv\"]\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(contours))]\n",
    "    colors = [[int(c*255) for c in color] for color in colors]\n",
    "    for i, contour in enumerate(contours):\n",
    "        x, y, w, h = cv.boundingRect(contour)\n",
    "        cv.rectangle(img, (x, y), (x+w, y+h), \n",
    "                    color=colors[i], thickness=2)\n",
    "        \n",
    "        # For the center of mass, we need to compute the moments of the contour\n",
    "        # https://docs.opencv.org/4.x/dd/d49/tutorial_py_contour_features.html\n",
    "        M = cv.moments(contour)\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        cv.circle(img, (cx, cy), 5, color=colors[i], thickness=-1)\n",
    "    return img\n",
    "\n",
    "img_in = cv.cvtColor(mask, cv.COLOR_GRAY2RGB)\n",
    "ret = draw_bounding_boxes(img_in, contours)\n",
    "results[\"Bounding boxes\"] = ret\n",
    "\n",
    "ret = draw_bounding_boxes(img, contours)\n",
    "results[\"Bounding boxes (as overlay)\"] = ret\n",
    "\n",
    "isp.show_image_grid(results, figsize=(10, 9), suppress_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Measuring components**\n",
    "In the following, we demonstrate how to compute various metrics – such as area, perimeter, and circularity – for each individual component (in this case, each red blood cell).\n",
    "\n",
    "We use the function `cv.connectedComponentsWithStats()` for this purpose. It returns four values:\n",
    "1. `num_labels`: The total number of connected components found (including the background).\n",
    "2. `labels`: A labeled image where each pixel value indicates the component it belongs to.\n",
    "3. `stats`: An array where each row contains statistics for one component, including   \n",
    "the bounding box (x, y, width, height) and the area\n",
    "4. `centroids`: The (x, y) coordinates of the centroid (center of mass) for each component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = cv.connectedComponentsWithStats(mask, connectivity=8)\n",
    "num_labels, labels, stats, centroids = ret\n",
    "\n",
    "df = pd.DataFrame(stats, columns=[\"x\", \"y\", \"width\", \"height\", \"area\"])\n",
    "df[\"centroid_x\"] = centroids[:, 0]\n",
    "df[\"centroid_y\"] = centroids[:, 1]\n",
    "df[\"label\"] = np.arange(num_labels)\n",
    "df[\"label\"] = df[\"label\"].astype(np.uint8)\n",
    "\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convex hull**\n",
    "\n",
    "A convex object is a set of points such that the line segment connecting *any two points* in the set lies entirely within the set. See below for examples of convex and non-convex shapes.\n",
    "\n",
    "\n",
    "<img src=\"../data/doc/convexity.svg\" style=\"width:60%;\">  \n",
    "\n",
    "**Figure**: Illustration of non-convex (left) and convex shapes (middle and right). Source: [Link](https://d2l.ai/index.html)\n",
    "\n",
    "The **convex hull** of a geometric object (or a set of points) is the smallest convex shape that fully encloses the object. It can be computed in OpenCV using the `cv.convexHull()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the convex hull for a binary mask\n",
    "mask_text = cv.imread(\"../data/images/word-ice-cream.png\", cv.IMREAD_GRAYSCALE)\n",
    "mask_text = 255 - mask_text  # Invert the mask\n",
    "contours_text, _ = cv.findContours(mask_text, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "drawing = cv.cvtColor(mask_text, cv.COLOR_GRAY2RGB)\n",
    "color = (255, 0, 0)\n",
    "for i in range(len(contours_text)):\n",
    "    hull = cv.convexHull(contours_text[i])\n",
    "    cv.drawContours(drawing, [hull], 0, color=color, thickness=1)\n",
    "\n",
    "isp.show_image_chain([mask_text, drawing], ncols=1,\n",
    "                     titles=[\"Input\", \"Convex hull\"], \n",
    "                     suppress_info=True, \n",
    "                     figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our initial dataset, the convex hulls look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the convex hull for a binary mask\n",
    "drawing = img.copy()\n",
    "color = (255, 255, 0)\n",
    "for i in range(len(contours)):\n",
    "    hull = cv.convexHull(contours[i])\n",
    "    cv.drawContours(drawing, [hull], 0, color=color, thickness=2)\n",
    "\n",
    "isp.show_image_chain([mask, drawing], \n",
    "                     titles=[\"Input\", \"Convex hull\"], \n",
    "                     suppress_info=True, \n",
    "                     figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Distance transform**\n",
    "\n",
    "The distance transform computes the distance from each foreground pixel to the nearest background (zero-valued) pixel.  It is typically applied to binary images and produces a grayscale image where pixel values represent distances. This transformation is useful in tasks such as shape analysis, object separation, and skeletonization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Input 1\"] = mask\n",
    "results[\"Input 2\"] = mask_text\n",
    "\n",
    "# Compute the distance transform\n",
    "dist_transform = cv.distanceTransform(mask, cv.DIST_L2, cv.DIST_MASK_PRECISE)\n",
    "dist_transform /= dist_transform.max()\n",
    "results[\"Distance transform 1\"] = dist_transform\n",
    "\n",
    "dist_transform = cv.distanceTransform(mask_text, cv.DIST_L2, cv.DIST_MASK_PRECISE)\n",
    "dist_transform /= dist_transform.max()\n",
    "results[\"Distance transform 2\"] = dist_transform\n",
    "\n",
    "isp.show_image_grid(results, ncols=2, figsize=(10, 5), \n",
    "                    suppress_info=True, shape=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hole filling**\n",
    "\n",
    "\n",
    "Hole filling is a common operation used to close internal gaps or holes within objects in a binary image. In this context, holes are background pixels that are completely enclosed by foreground pixels. OpenCV provides several approaches for hole filling, two of which are demonstrated here.\n",
    "\n",
    "Note that the first method, which is based on morphological operations, is effective for closing relatively small gaps. The second method, which relies on contours, is also capable of filling larger holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Morphological operations (only works for small holes)\n",
    "kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (3, 3))\n",
    "mask_filled1 = cv.morphologyEx(mask_distorted, cv.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "# Method 2: Contour filling\n",
    "mask_filled2 = mask_distorted.copy()\n",
    "contour, hierarchy = cv.findContours(mask_distorted, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE)\n",
    "for cnt in contour:\n",
    "    cv.drawContours(mask_filled2,[cnt],0,255,-1)\n",
    "\n",
    "# Visalize the results\n",
    "isp.show_image_chain([mask_distorted, mask_filled1, mask_filled2], \n",
    "                     titles=[\"Noisy mask (input)\", \"Filled (morphology)\", \"Filled (contour)\"],\n",
    "                     figsize=(9, 5),\n",
    "                     suppress_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Flood fill**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to fill a region in the mask with a specific gray level (e.g., 255), starting from a given pixel—but only if that pixel is connected to the top-left corner. This can be achieved using the flood fill algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_point = (30, 30)\n",
    "\n",
    "# Flood fill\n",
    "result1 = np.zeros_like(mask_distorted, dtype=np.uint8)\n",
    "mask_constraints = cv.copyMakeBorder(mask_distorted, 1, 1, 1, 1,\n",
    "                                     cv.BORDER_CONSTANT, value=0)\n",
    "cv.floodFill(result1, mask_constraints, seed_point, 255)\n",
    "\n",
    "# Visualize the seed point as a red dot\n",
    "mask_filled_with_seed = cv.cvtColor(result1, cv.COLOR_GRAY2RGB)\n",
    "cv.circle(mask_filled_with_seed, seed_point, 7, [255,0,0], -1)\n",
    "isp.show_image_chain([mask_distorted, mask_filled_with_seed, None], \n",
    "                      titles=[\"Input\", \"Filled mask (flood fill)\", None],\n",
    "                      suppress_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can use a second mask to restrict the flood fill.\n",
    "result2 = np.zeros_like(mask_distorted, dtype=np.uint8)\n",
    "\n",
    "mask_constraints = cv.copyMakeBorder(mask_distorted, 1, 1, 1, 1,\n",
    "                                     cv.BORDER_CONSTANT, value=0)\n",
    "lw = 10  # Line width\n",
    "mask_constraints = cv.circle(mask_constraints, (150, 150), 125, 255, lw)\n",
    "mask_constraints = cv.rectangle(mask_constraints, (250, 100), (450, 300), 255, lw)\n",
    "mask_constraints = cv.ellipse(mask_constraints, (500, 330), (100, 80), 30, 0, 360, 255, lw)\n",
    "cv.floodFill(result2, mask_constraints, (0, 0), 255)\n",
    "\n",
    "# Visualize the seed point as a red dot\n",
    "mask_filled_with_seed = cv.cvtColor(result2, cv.COLOR_GRAY2RGB)\n",
    "cv.circle(mask_filled_with_seed, seed_point, 7, [255,0,0], -1)\n",
    "isp.show_image_chain([mask_distorted, mask_constraints, mask_filled_with_seed], \n",
    "                      titles=[\"Input\", \"Mask\", \"Filled mask (flood fill)\"],\n",
    "                      suppress_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous example, the flood fill is constrained not only by the mask but also by the image content. As a result, it does not fill the entire image. Only the pixels that are connected to the seed point (= have the same value) are filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Thinning**\n",
    "Thinning (or skeletonization) is the process of reducing a binary image to a\n",
    "skeleton representation. The skeleton is a thin representation of the object\n",
    "that is useful for shape analysis. It can be computed using the Zhang-Suen\n",
    "algorithm, which is implemented in the opencv-contrib-python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the skeleton (requires the opencv-contrib-python package)\n",
    "if (not hasattr(cv, \"ximgproc\")) or (not hasattr(cv.ximgproc, \"thinning\")):\n",
    "    raise RuntimeError(\"This version of OpenCV does not support thinning.\")\n",
    "\n",
    "thinned = cv.ximgproc.thinning(mask_text)\n",
    "ret = cv.cvtColor(mask_text, cv.COLOR_GRAY2RGB)\n",
    "ret[thinned == 255] = [31, 41, 255]\n",
    "\n",
    "isp.show_image_chain([mask_text, ret], ncols=1, \n",
    "                     titles=[\"Input (inverted mask)\", \n",
    "                             \"Skeletonization of the mask\"], \n",
    "                     suppress_info=True, figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinning can be used not only to reduce a binary image to its skeleton but also to identify paths that are equidistant from the object boundaries. An example is shown below. Note that in this case, the thinning is applied to the background (i.e., the inverted mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_inv = 255 - mask\n",
    "thinned = cv.ximgproc.thinning(mask_inv)\n",
    "ret = img.copy()\n",
    "ret[thinned == 255] = [255, 0, 255]\n",
    "\n",
    "isp.show_image_chain([mask, ret], ncols=2, \n",
    "                     titles=[\"Input (inverted mask)\", \n",
    "                             \"Skeletonization of the inverted mask\"], \n",
    "                     suppress_info=True, figsize=(7, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Voronoi tessellation and Delaunay triangulation** (advanced topic)\n",
    "\n",
    "Voronoi tessellation and Delaunay triangulation are geometric constructions often used in image processing, computer vision, and computational geometry.\n",
    "\n",
    "A **Voronoi tessellation** partitions a plane into regions based on the distance to a set of seed points. Each region contains all the points closer to its seed than to any other. This is useful for modeling influence zones or proximity. The Voronoi tessellation is conceptually related to the nearest neighbor rule – each Voronoi cell represents the region where its seed is the nearest neighbor to any point within that region.\n",
    "\n",
    "A **Delaunay triangulation** connects points such that no point lies inside the circumcircle of any triangle created in this process. It is often used for mesh generation, interpolation, and feature extraction. The Delaunay triangulation and the Voronoi tessellation are dual operations. Each edge in the Delaunay triangulation corresponds to a shared boundary between two Voronoi regions.\n",
    "\n",
    "In the following, let's operate with the centroids of the cell blobs we already have calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask = cv.cvtColor(mask, cv.COLOR_GRAY2RGB)\n",
    "    \n",
    "# Compute the Voronoi diagram\n",
    "height, width = mask.shape\n",
    "subdiv = cv.Subdiv2D((0, 0, width, height))\n",
    "for i in range(1, len(df)):\n",
    "    x = int(df[\"centroid_x\"][i])\n",
    "    y = int(df[\"centroid_y\"][i])\n",
    "    subdiv.insert((x, y))\n",
    "\n",
    "image_voronoi = image_mask.copy()\n",
    "facets, centers = subdiv.getVoronoiFacetList([])\n",
    "for i in range(len(facets)):\n",
    "    if len(facets[i]) == 0:\n",
    "        continue\n",
    "    # Draw the facets\n",
    "    #cv.fillConvexPoly(image_mask, np.array(facets[i], dtype=np.int32), [0, 255, 0])\n",
    "    cv.polylines(image_voronoi, [np.array(facets[i], dtype=np.int32)], \n",
    "                 isClosed=True, color=[255, 0, 255], thickness=2)\n",
    "    \n",
    "image_delaunay = image_mask.copy()\n",
    "triangle_list = subdiv.getTriangleList()\n",
    "for i in range(len(triangle_list)):\n",
    "    pt1 = (int(triangle_list[i][0]), int(triangle_list[i][1]))\n",
    "    pt2 = (int(triangle_list[i][2]), int(triangle_list[i][3]))\n",
    "    pt3 = (int(triangle_list[i][4]), int(triangle_list[i][5]))\n",
    "    cv.line(image_delaunay, pt1, pt2, [255, 255, 0], 2)\n",
    "    cv.line(image_delaunay, pt2, pt3, [255, 255, 0], 2)\n",
    "    cv.line(image_delaunay, pt3, pt1, [255, 255, 0], 2)\n",
    "    # Draw the centroids\n",
    "    cv.circle(image_delaunay, (int(round(pt1[0])), int(round(pt1[1]))), 5, [255, 0, 0], -1)\n",
    "    \n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    x = int(df[\"centroid_x\"][i])\n",
    "    y = int(df[\"centroid_y\"][i])\n",
    "    cv.circle(image_voronoi, (x, y), 5, [255, 0, 0], -1)\n",
    "    cv.circle(image_delaunay, (x, y), 5, [255, 0, 0], -1)\n",
    "    \n",
    "isp.show_image_chain(images=[mask, image_voronoi, image_delaunay], \n",
    "                     titles=[\"Input\", \n",
    "                             \"Voronoi tessellation\", \n",
    "                             \"Delaunay triangulation\"],\n",
    "                     suppress_info=True,\n",
    "                     figsize=(9, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the edges of the Voronoi tessellation are perpendicular to the corresponding edges of the Delaunay triangulation. What a beautiful result!\n",
    "\n",
    "Voronoi diagrams and Delaunay triangulations are great for analyzing spatial relationships between points. Their ability to model proximity and connectivity makes them valuable in applications ranging from object detection to biological structure modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "## **Evaluation metrics**\n",
    "\n",
    "**TODO**! Intersection over Union, Dice score\n",
    "--->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-isp-fs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
